# SPDX-FileCopyrightText: Magenta ApS <https://magenta.dk>
# SPDX-License-Identifier: MPL-2.0
import asyncio
import datetime
import re
from typing import Any
from uuid import UUID

import structlog
from fastapi import APIRouter
from fastapi import BackgroundTasks
from fastapi import FastAPI
from fastapi import HTTPException
from fastapi import Response
from fastramqpi.events import GraphQLEvents
from fastramqpi.events import Listener
from fastramqpi.events import Namespace
from fastramqpi.main import FastRAMQPI
from fastramqpi.metrics import dipex_last_success_timestamp  # a Prometheus `Gauge`
from fastramqpi.os2mo_dar_client import AsyncDARClient
from more_itertools import first
from more_itertools import one
from sdclient.client import SDClient
from sdclient.exceptions import SDRootElementNotFound
from sdclient.requests import GetDepartmentRequest
from sdclient.responses import Department
from sqlalchemy import Engine
from starlette.status import HTTP_200_OK
from starlette.status import HTTP_422_UNPROCESSABLE_ENTITY
from starlette.status import HTTP_500_INTERNAL_SERVER_ERROR

from sdtoolplus.job_positions import sync_professions
from sdtoolplus.roots import ensure_sd_institution_units_and_unknown_unit

from . import depends
from .addresses import AddressFixer
from .app import App
from .autogenerated_graphql_client import ClassFilter
from .autogenerated_graphql_client import EngagementFilter
from .autogenerated_graphql_client import EventSendInput
from .autogenerated_graphql_client import FacetFilter
from .autogenerated_graphql_client import GraphQLClient
from .config import SDToolPlusSettings
from .db.engine import get_engine
from .db.rundb import Status
from .db.rundb import delete_last_run
from .db.rundb import get_status
from .db.rundb import persist_status
from .events import EmploymentGraphQLEvent
from .events import PersonGraphQLEvent
from .events import router as events_router
from .events import sd_amqp_lifespan
from .exceptions import EngagementSyncTemporarilyDisabled
from .exceptions import UnknownNYLevel
from .middleware import ExceptionLoggerMiddleware
from .middleware import RequestIDMiddleware
from .minisync.api import minisync_router
from .mo_class import MOOrgUnitLevelMap
from .models import EngagementSyncPayload
from .models import OrgGraphQLEvent
from .models import OrgUnitSyncPayload
from .models import PersonSyncPayload
from .sd.person import get_all_sd_persons
from .sd.person import get_sd_person_engagements
from .sync.engagement import queue_mo_engagements_for_sd_unit_sync
from .sync.engagement import sync_engagement
from .sync.org_unit import sync_ou
from .sync.person import sync_person
from .tree_tools import tree_as_string

logger = structlog.stdlib.get_logger()


def _configure_listeners(settings: SDToolPlusSettings) -> list[Listener]:
    listeners: list[Listener] = []
    if not settings.event_based_sync:
        return listeners
    if not settings.disable_sd_events:
        if not settings.disable_sd_ou_events:
            listeners.append(
                Listener(
                    namespace="sd",
                    user_key="org",
                    routing_key="org",
                    path="/events/sd/org",
                    # The org unit handler requeues its parent/children with priority
                    # +1/-1. We don't understand the implications of doing this in
                    # parallel.
                    parallelism=1,
                )
            )
        if not settings.disable_sd_person_events:
            listeners.append(
                Listener(
                    namespace="sd",
                    user_key="person",
                    routing_key="person",
                    path="/events/sd/person",
                    parallelism=3,
                )
            )
        if not settings.disable_sd_engagement_events:
            listeners.append(
                Listener(
                    namespace="sd",
                    user_key="employment",
                    routing_key="employment",
                    path="/events/sd/employment",
                    parallelism=3,
                )
            )
    if not settings.disable_mo_events:
        if not settings.disable_mo_ou_events:
            listeners.append(
                Listener(
                    namespace="mo",
                    user_key="org_unit",
                    routing_key="org_unit",
                    path="/events/mo/org-unit",
                    # The org unit handler requeues its parent/children with priority
                    # +1/-1. We don't understand the implications of doing this in
                    # parallel.
                    parallelism=1,
                )
            )
        if not settings.disable_mo_person_events:
            listeners.append(
                Listener(
                    namespace="mo",
                    user_key="person",
                    routing_key="person",
                    path="/events/mo/person",
                    parallelism=3,
                )
            )
        if not settings.disable_mo_engagement_events:
            listeners.append(
                Listener(
                    namespace="mo",
                    user_key="engagement",
                    routing_key="engagement",
                    path="/events/mo/engagement",
                    parallelism=3,
                )
            )

    return listeners


async def run_db_start_operations(
    engine: Engine, dry_run: bool, response: Response
) -> dict | None:
    if dry_run:
        return None

    logger.info("Checking RunDB status...")
    status_last_run = await get_status(engine)
    if not status_last_run == Status.COMPLETED:
        logger.warn("Previous run did not complete successfully!")
        response.status_code = HTTP_500_INTERNAL_SERVER_ERROR
        return {"msg": "Previous run did not complete successfully!"}
    logger.info("Previous run completed successfully")

    await persist_status(engine, Status.RUNNING)

    return None


async def run_db_end_operations(engine: Engine, dry_run: bool) -> None:
    if not dry_run:
        await persist_status(engine, Status.COMPLETED)
    dipex_last_success_timestamp.set_to_current_time()


async def background_run(
    settings: SDToolPlusSettings,
    engine: Engine,
    inst_ids: list[str],
    org_unit: UUID | None = None,
    dry_run: bool = False,
) -> None:
    """
    Run org tree sync in background for all institutions.

    Args:
        settings: the SDToolPlusSettings
        engine: the SQLAlchemy DB engine
        inst_ids: list of the SD InstitutionIdentifiers
        org_unit: if not None, only run for this unit
        dry_run: if True, no changes will be written in MO
    """
    sdtoolplus: App = App(settings, first(inst_ids))

    for ii in inst_ids:
        logger.info("Starting background run", inst_id=ii)
        sdtoolplus.set_inst_id(ii)
        async for org_unit_node, mutation, result in sdtoolplus.execute(
            org_unit=org_unit, dry_run=dry_run
        ):
            logger.info(
                "Processed unit",
                org_unit_name=org_unit_node.name,
                org_unit_uuid=str(org_unit_node.uuid),
            )

        logger.info("Finished background run", inst_id=ii)

        # Send email notifications for illegal moves
        if settings.email_notifications_enabled and not dry_run:
            sdtoolplus.send_email_notification()

    await run_db_end_operations(engine, dry_run)
    logger.info("Run completed!")


def create_fastramqpi() -> FastRAMQPI:
    settings = SDToolPlusSettings()

    fastramqpi = FastRAMQPI(
        application_name="os2mo-sdtool-plus",
        settings=settings.fastramqpi,
        graphql_client_cls=GraphQLClient,
        graphql_version=25,
        graphql_events=GraphQLEvents(
            declare_namespaces=[
                Namespace(name="sd"),
            ],
            declare_listeners=_configure_listeners(settings),
        ),
    )
    fastramqpi.add_context(settings=settings)

    engine = get_engine(settings)
    fastramqpi.add_context(engine=engine)

    sd_client = SDClient(
        sd_username=settings.sd_username,
        sd_password=settings.sd_password.get_secret_value(),
        url_subpath_xml_endpoints=settings.sd_url_subpath_xml_endpoints,
        url_subpath_json_endpoints=settings.sd_url_subpath_json_endpoints,
    )
    fastramqpi.add_context(sd_client=sd_client)

    if settings.ensure_sd_institution_units:
        fastramqpi.add_lifespan_manager(
            ensure_sd_institution_units_and_unknown_unit(
                settings=settings,
                sd_client=sd_client,
                context=fastramqpi.get_context(),
            ),
            priority=1100,
        )
    if settings.sd_amqp is not None:
        fastramqpi.add_lifespan_manager(
            sd_amqp_lifespan(
                settings=settings.sd_amqp, context=fastramqpi.get_context()
            ),
            priority=1200,
        )

    fastapi_router = APIRouter()

    @fastapi_router.get("/tree/mo")
    async def print_mo_tree(settings: depends.Settings) -> str:
        """
        For debugging problems. Prints the part of the MO tree that
        should be compared to the SD tree.
        """
        sdtoolplus: App = App(settings)
        mo_tree = sdtoolplus.get_mo_tree()
        return tree_as_string(mo_tree)

    @fastapi_router.get("/tree/sd")
    async def print_sd_tree(settings: depends.Settings) -> str:
        """
        For debugging problems. Prints the SD tree.
        """
        sdtoolplus: App = App(settings)
        mo_org_unit_level_map = MOOrgUnitLevelMap(sdtoolplus.session)
        sd_tree = await sdtoolplus.get_sd_tree(mo_org_unit_level_map)
        return tree_as_string(sd_tree)

    @fastapi_router.get("/rundb/status")
    async def rundb_get_status(engine: depends.Engine) -> int:
        """
        Get the RunDB status and return a job-runner.sh (curl) friendly integer
        status.

        Returns:
            0 if status is "completed", 1 if status is "running" and 3 in case of
            an error.
        """
        try:
            status = await get_status(engine)
            return 0 if status == Status.COMPLETED else 1
        except Exception:
            return 3

    @fastapi_router.post("/rundb/delete-last-run")
    async def rundb_delete_last_run(engine: depends.Engine):
        await delete_last_run(engine)
        return {"msg": "Last run deleted"}

    @fastapi_router.post("/job-functions/sync")
    async def sync_job_functions(
        sd_client: depends.SDClient,
        graphql_client: depends.GraphQLClient,
        institution_identifier: str,
        force_class_start_date: datetime.date | None = None,
    ) -> None:
        await sync_professions(
            sd_client, graphql_client, institution_identifier, force_class_start_date
        )

    @fastapi_router.post("/trigger", status_code=HTTP_200_OK)
    async def trigger(
        settings: depends.Settings,
        engine: depends.Engine,
        response: Response,
        org_unit: UUID | None = None,
        inst_id: str | None = None,
        dry_run: bool = False,
    ) -> list[dict] | dict:
        logger.info("Starting run", org_unit=str(org_unit), dry_run=dry_run)

        run_db_start_operations_resp = await run_db_start_operations(
            engine, dry_run, response
        )
        if run_db_start_operations_resp is not None:
            return run_db_start_operations_resp

        sdtoolplus: App = App(settings, inst_id)

        results: list[dict] = [
            {
                "type": mutation.__class__.__name__,
                "unit": repr(org_unit_node),
                "mutation_result": str(result),
            }
            async for org_unit_node, mutation, result in sdtoolplus.execute(
                org_unit=org_unit, dry_run=dry_run
            )
        ]
        logger.info("Finished adding or updating org unit objects")

        # Send email notifications for illegal moves
        if settings.email_notifications_enabled and not dry_run:
            sdtoolplus.send_email_notification()

        await run_db_end_operations(engine, dry_run)
        logger.info("Run completed!")

        return results

    @fastapi_router.post("/trigger-all-inst-ids", status_code=HTTP_200_OK)
    async def trigger_all_inst_ids(
        settings: depends.Settings,
        engine: depends.Engine,
        response: Response,
        background_tasks: BackgroundTasks,
        org_unit: UUID | None = None,
        inst_id: str | None = None,
        dry_run: bool = False,
    ) -> dict[str, str]:
        logger.info("Starting run", org_unit=str(org_unit), dry_run=dry_run)

        run_db_start_operations_resp = await run_db_start_operations(
            engine, dry_run, response
        )
        if run_db_start_operations_resp is not None:
            return run_db_start_operations_resp

        if inst_id is not None:
            inst_ids = [inst_id]
        else:
            assert settings.mo_subtree_paths_for_root is not None
            inst_ids = list(settings.mo_subtree_paths_for_root.keys())

        background_tasks.add_task(
            background_run, settings, engine, inst_ids, org_unit, dry_run
        )

        return {"msg": "Org tree sync started in background"}

    @fastapi_router.post("/trigger/addresses", status_code=HTTP_200_OK)
    async def trigger_addresses(
        settings: depends.Settings,
        engine: depends.Engine,
        response: Response,
        gql_client: depends.GraphQLClient,
        org_unit: UUID | None = None,
        inst_id: str | None = None,
        dry_run: bool = False,
    ) -> list[dict] | dict:
        logger.info("Starting address run", org_unit=str(org_unit), dry_run=dry_run)

        run_db_start_operations_resp = await run_db_start_operations(
            engine, dry_run, response
        )
        if run_db_start_operations_resp is not None:
            return run_db_start_operations_resp

        addr_fixer = AddressFixer(
            gql_client,
            SDClient(
                settings.sd_username,
                settings.sd_password.get_secret_value(),
            ),
            AsyncDARClient(),
            settings,
            inst_id if inst_id is not None else settings.sd_institution_identifier,
        )

        results: list[dict] = [
            {
                "address_operation": operation.value,
                "address_type": addr.address_type.user_key,
                "unit": repr(org_unit_node),
                "address": addr.value,
            }
            async for operation, org_unit_node, addr in addr_fixer.fix_addresses(
                org_unit, dry_run
            )
        ]
        logger.info("Finished adding or updating org unit objects")

        await run_db_end_operations(engine, dry_run)
        logger.info("Run completed!")

        return results

    @fastapi_router.post("/timeline/sync/person", status_code=HTTP_200_OK)
    async def timeline_sync_person(
        sd_client: depends.SDClient,
        gql_client: depends.GraphQLClient,
        settings: depends.Settings,
        payload: PersonSyncPayload,
    ) -> dict:
        """Sync the person with the given CPR from the given institution identifier."""
        await sync_person(
            sd_client=sd_client,
            gql_client=gql_client,
            settings=settings,
            institution_identifier=payload.institution_identifier,
            cpr=payload.cpr,
        )

        return {"msg": "success"}

    @fastapi_router.post("/timeline/sync/person/all")
    async def sync_all_persons(
        sd_client: depends.SDClient,
        graphql_client: depends.GraphQLClient,
        institution_identifier: str,
        only_active_persons: bool = False,
    ) -> dict:
        """
        Sync all persons in SD
        """
        logger.info("Syncing all SD persons")

        sd_persons = await get_all_sd_persons(
            sd_client=sd_client,
            institution_identifier=institution_identifier,
            effective_date=datetime.date.today(),
            only_active_persons=only_active_persons,
        )

        events = [
            EventSendInput(
                namespace="sd",
                routing_key="person",
                subject=PersonGraphQLEvent(
                    institution_identifier=institution_identifier,
                    cpr=person.cpr,
                ).json(),
            )
            for person in sd_persons
        ]
        logger.info(
            "Syncing persons",
            events=len(events),
        )
        for e in events:
            await graphql_client.send_event(input=e)

        logger.info(f"Done queueing sync all SD persons in {institution_identifier}")

        return {"msg": f"{len(events)} person events queued"}

    @fastapi_router.post("/timeline/sync/engagement", status_code=HTTP_200_OK)
    async def timeline_sync_engagement(
        settings: depends.Settings,
        sd_client: depends.SDClient,
        gql_client: depends.GraphQLClient,
        payload: EngagementSyncPayload,
        dry_run: bool = False,
    ) -> dict:
        if not settings.recalc_mo_unit_when_sd_employment_moved:
            raise EngagementSyncTemporarilyDisabled()

        await sync_engagement(
            sd_client=sd_client,
            gql_client=gql_client,
            institution_identifier=payload.institution_identifier,
            cpr=payload.cpr,
            employment_identifier=payload.employment_identifier,
            settings=settings,
            dry_run=dry_run,
        )
        return {"msg": "success"}

    @fastapi_router.post("/timeline/sync/engagement/all/sd", status_code=HTTP_200_OK)
    async def full_timeline_sync_sd_engagements(
        sd_client: depends.SDClient,
        gql_client: depends.GraphQLClient,
        institution_identifier: str,
        only_active_persons: bool = False,
        dry_run: bool = False,
    ) -> dict:
        """
        Sync engagements of all SD persons, i.e.

        1) Read all persons from SD
        2) Loop through these and for each get a list of their SD employments
        3) Loop through the list of SD employments and queue these for sync
        """
        logger.info(f"Syncing all SD employments in {institution_identifier}")

        sd_persons = await get_all_sd_persons(
            sd_client=sd_client,
            institution_identifier=institution_identifier,
            effective_date=datetime.date.today(),
            only_active_persons=only_active_persons,
        )

        if dry_run:
            logger.info(
                f"Dry-run. Would create engagement events for {len(sd_persons)} persons"
            )
            return {"msg": "success"}
        for person in sd_persons:
            try:
                res = await get_sd_person_engagements(
                    sd_client=sd_client,
                    institution_identifier=institution_identifier,
                    cpr=person.cpr,
                )
                logger.info("Found engagements", engagements=res)
            except SDRootElementNotFound:
                logger.info(
                    "Person could not be found in sd",
                    institution_identifier=institution_identifier,
                    person=person,
                )
                continue

            for e in one(res.Person).Employment:
                event = EventSendInput(
                    namespace="sd",
                    routing_key="employment",
                    subject=EmploymentGraphQLEvent(
                        institution_identifier=institution_identifier,
                        cpr=person.cpr,
                        employment_identifier=e.EmploymentIdentifier,
                    ).json(),
                )
                await gql_client.send_event(input=event)

        logger.info(
            f"Done queueing sync for all SD employments in {institution_identifier}"
        )
        return {"msg": "success"}

    @fastapi_router.post("/timeline/sync/engagement/all/mo", status_code=HTTP_200_OK)
    async def full_timeline_sync_mo_engagements(
        gql_client: depends.GraphQLClient,
        engagement_uuid: UUID | None = None,
        limit: int = 500,
    ) -> dict:
        """
        Sync all engagements in MO (and only the ones that already exist in MO).
        """
        logger.info("Syncing all MO engagements")

        # Get actor UUID for application
        me = await gql_client.get_actor()

        # Refresh all MO engagements
        eng_filter: dict[str, Any] = {
            "engagement_type": ClassFilter(
                facet=FacetFilter(user_keys=["engagement_type"]),
                user_keys=[
                    # User keys from the old SD integration
                    "timeløn",
                    "månedsløn",
                    # User keys from the new SD integration
                    "timelønnet",
                    "deltid",
                    "fuldtid",
                ],
            ),
            "from_date": None,
            "to_date": None,
        }
        if engagement_uuid is not None:
            eng_filter["uuids"] = [engagement_uuid]

        next_cursor = None
        engagements_refreshed = 0
        while True:
            batch = await gql_client.refresh_engagements(
                cursor=next_cursor,
                limit=limit,
                filter=EngagementFilter(**eng_filter),
                owner=me.actor.uuid,
            )
            next_cursor = batch.page_info.next_cursor

            engagements_refreshed += len(batch.objects)
            logger.info("Engagements refreshed", n=engagements_refreshed)

            if next_cursor is None:
                break

        logger.info("Done queueing all MO engagements")

        return {"msg": "success"}

    @fastapi_router.post("/timeline/sync/ou", status_code=HTTP_200_OK)
    async def timeline_sync_ou(
        settings: depends.Settings,
        sd_client: depends.SDClient,
        gql_client: depends.GraphQLClient,
        payload: OrgUnitSyncPayload,
        dry_run: bool = False,
    ) -> dict:
        await sync_ou(
            sd_client=sd_client,
            gql_client=gql_client,
            institution_identifier=payload.institution_identifier,
            org_unit=payload.org_unit,
            settings=settings,
            priority=9000,
            dry_run=dry_run,
        )
        return {"msg": "success"}

    @fastapi_router.post(
        "/timeline/sync/engagement/mo-sd-units", status_code=HTTP_200_OK
    )
    async def timeline_sync_engagement_mo_sd_unit(
        settings: depends.Settings,
        gql_client: depends.GraphQLClient,
        background_tasks: BackgroundTasks,
        cpr: str | None = None,
        dry_run: bool = False,
    ) -> dict:
        if settings.recalc_mo_unit_when_sd_employment_moved:
            raise HTTPException(
                status_code=HTTP_422_UNPROCESSABLE_ENTITY,
                detail="MO SD unit sync not allowed when RECALC_MO_UNIT_WHEN_SD_EMPLOYMENT_MOVED is true",
            )

        background_tasks.add_task(
            queue_mo_engagements_for_sd_unit_sync,
            gql_client=gql_client,
            settings=settings,
            cpr=cpr,
            dry_run=dry_run,
        )

        return {"msg": "Sync started in background"}

    @fastapi_router.post("/timeline/sync/ou/all", status_code=HTTP_200_OK)
    async def full_timeline_sync_ous(
        sd_client: depends.SDClient,
        gql_client: depends.GraphQLClient,
        institution_identifier: str,
        dry_run: bool = False,
    ) -> dict:
        logger.info(f"Syncing all SD units in {institution_identifier}")
        # TODO: This only works when all unit_levels are integers
        ny_regex = re.compile(r"NY(\d)-niveau")

        def priority_from_level(department: Department) -> int:
            # Set priority on org_unit events such that top-level units are imported first.
            DEFAULT_PRIORITY = 10_000
            if department.DepartmentLevelIdentifier == "Afdelings-niveau":
                return DEFAULT_PRIORITY
            match = ny_regex.match(department.DepartmentLevelIdentifier)
            if not match:
                raise UnknownNYLevel()
            priority = DEFAULT_PRIORITY - int(one(match.groups()))
            return priority

        departments = await asyncio.to_thread(
            sd_client.get_department,
            GetDepartmentRequest(
                InstitutionIdentifier=institution_identifier,
                ActivationDate=datetime.datetime.now(),
                DeactivationDate=datetime.datetime.now(),
                DepartmentNameIndicator=False,
                UUIDIndicator=True,
                PostalAddressIndicator=False,
            ),
        )
        if dry_run:
            logger.info(
                f"Dry-run. Would create {len(departments.Department)} org_unit events"
            )
            return {"msg": "success"}

        events = []
        for d in departments.Department:
            try:
                priority = priority_from_level(d)
            except UnknownNYLevel:
                logger.warning(
                    "Unknown NY level. Skipping...",
                    org_unit=str(d.DepartmentUUIDIdentifier),
                    level=d.DepartmentLevelIdentifier,
                )
                continue
            events.append(
                EventSendInput(
                    namespace="sd",
                    routing_key="org",
                    subject=OrgGraphQLEvent(
                        institution_identifier=institution_identifier,
                        org_unit=d.DepartmentUUIDIdentifier,
                    ).json(),
                    priority=priority,
                )
            )

        logger.info("Syncing units", events=len(events))
        for e in events:
            await gql_client.send_event(input=e)

        logger.info(f"Done queueing sync all SD units in {institution_identifier}")
        return {"msg": f"{len(events)} OU events queued"}

    app = fastramqpi.get_app()
    app.include_router(fastapi_router)
    app.include_router(minisync_router)
    app.include_router(events_router)

    # ExceptionLoggerMiddleware must be installed before RequestIDMiddleware to
    # ensure that exception logs receive the request-id
    app.add_middleware(ExceptionLoggerMiddleware)
    app.add_middleware(RequestIDMiddleware)

    return fastramqpi


def create_app() -> FastAPI:
    fastramqpi = create_fastramqpi()
    return fastramqpi.get_app()
